{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9R-Y-F1owP3",
        "outputId": "a993a596-27dd-48b3-c79d-aaa7f4b6c451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2294280059.py:60: FutureWarning: Downcasting behavior in Series and DataFrame methods 'where', 'mask', and 'clip' is deprecated. In a future version this will not infer object dtypes or cast all-round floats to integers. Instead call result.infer_objects(copy=False) for object inference, or cast round floats explicitly. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  vals = vals.clip(low,high)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline AUC (LogReg): 0.9045863536869\n",
            "Figures and summaries saved in /content/drive/My Drive/Data Corsair/loan_pitch_figures\n",
            "\n",
            "=== Running Checkpoint 2 (v3) ===\n",
            "Checkpoint 2 artifacts will be saved in: /content/drive/My Drive/Data Corsair/loan_checkpoint2_figures\n",
            "\n",
            "1. Building Enhanced Preprocessing Pipeline...\n",
            "\n",
            "2. Defining 9 models for comparison...\n",
            "Defined 9 models.\n",
            "\n",
            "3. Running baseline model comparison (this may take a minute)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ... Logistic Regression complete. AUC: 0.9062 (Time: 20.97s)\n",
            "  ... k-Nearest Neighbors (k-NN) complete. AUC: 0.5503 (Time: 6.03s)\n",
            "  ... Support Vector Machine (SVM) complete. AUC: 0.4969 (Time: 976.52s)\n",
            "  ... Gaussian Naive Bayes complete. AUC: 0.8862 (Time: 0.28s)\n",
            "  ... Decision Tree complete. AUC: 0.8258 (Time: 0.59s)\n",
            "  ... Random Forest complete. AUC: 0.9321 (Time: 5.54s)\n",
            "  ... AdaBoost complete. AUC: 0.9147 (Time: 3.41s)\n",
            "  ... Neural Network (MLP) complete. AUC: 0.7643 (Time: 11.34s)\n",
            "  ... XGBoost complete. AUC: 0.9488 (Time: 0.96s)\n",
            "\n",
            "--- Baseline Model Comparison (Sorted by AUC) ---\n",
            "| Model                        |    AUC |     F1 |   Precision |   Recall |   Accuracy |\n",
            "|:-----------------------------|-------:|-------:|------------:|---------:|-----------:|\n",
            "| XGBoost                      | 0.9488 | 0.7698 |      0.7342 |   0.8090 |     0.9311 |\n",
            "| Random Forest                | 0.9321 | 0.8037 |      0.9349 |   0.7048 |     0.9510 |\n",
            "| AdaBoost                     | 0.9147 | 0.6990 |      0.7927 |   0.6251 |     0.9234 |\n",
            "| Logistic Regression          | 0.9062 | 0.6057 |      0.4753 |   0.8347 |     0.8453 |\n",
            "| Gaussian Naive Bayes         | 0.8862 | 0.6043 |      0.6499 |   0.5647 |     0.8947 |\n",
            "| Decision Tree                | 0.8258 | 0.6986 |      0.6943 |   0.7030 |     0.9136 |\n",
            "| Neural Network (MLP)         | 0.7643 | 0.3466 |      0.4726 |   0.2737 |     0.8531 |\n",
            "| k-Nearest Neighbors (k-NN)   | 0.5503 | 0.0556 |      0.2587 |   0.0311 |     0.8493 |\n",
            "| Support Vector Machine (SVM) | 0.4969 | 0.2161 |      0.1389 |   0.4862 |     0.4978 |\n",
            "\n",
            "ANALYSIS: XGBoost and RF are top performers.\n",
            "The default MLP is strong but likely needs SMOTE and tuning.\n",
            "\n",
            "4. Proposing improvements: Hyperparameter tuning for top 2 models...\n",
            "\n",
            "--- Tuning Random Forest (this may take a few minutes) ---\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Best RF Params: {'clf__max_depth': 20, 'clf__min_samples_leaf': 4, 'clf__n_estimators': 200}\n",
            "\n",
            "--- Tuning XGBoost (this may take a few minutes) ---\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "Best XGB Params: {'clf__learning_rate': 0.1, 'clf__max_depth': 5, 'clf__n_estimators': 200}\n",
            "\n",
            "4c. Proposing improvement: Soft Voting Ensemble...\n",
            "Fitting Voting Classifier (this may take a minute)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "5. Evaluating tuned models and ensemble on validation set...\n",
            "\n",
            "--- Final Comparison: Tuned, Ensemble, and Baseline ---\n",
            "| Model                        |    AUC |     F1 |   Precision |   Recall |   Accuracy |\n",
            "|:-----------------------------|-------:|-------:|------------:|---------:|-----------:|\n",
            "| Tuned XGBoost                | 0.9545 | 0.7642 |      0.7055 |   0.8335 |     0.9268 |\n",
            "| XGBoost                      | 0.9488 | 0.7698 |      0.7342 |   0.8090 |     0.9311 |\n",
            "| Voting Ensemble              | 0.9479 | 0.7784 |      0.7644 |   0.7928 |     0.9357 |\n",
            "| Tuned Random Forest          | 0.9350 | 0.7961 |      0.8613 |   0.7401 |     0.9460 |\n",
            "| Random Forest                | 0.9321 | 0.8037 |      0.9349 |   0.7048 |     0.9510 |\n",
            "| Logistic Regression          | 0.9062 | 0.6057 |      0.4753 |   0.8347 |     0.8453 |\n",
            "| Neural Network (MLP)         | 0.7643 | 0.3466 |      0.4726 |   0.2737 |     0.8531 |\n",
            "| Support Vector Machine (SVM) | 0.4969 | 0.2161 |      0.1389 |   0.4862 |     0.4978 |\n",
            "\n",
            "DELIVERABLE: Enhanced results show the Voting Ensemble provides\n",
            "the best and most robust performance.\n",
            "\n",
            "Generating final ROC curve plot...\n",
            "Final ROC plot saved to /content/drive/My Drive/Data Corsair/loan_checkpoint2_figures\n",
            "\n",
            "==================================================\n",
            "6. Future Plans for Final Project\n",
            "==================================================\n",
            "1. Advanced Feature Engineering: Create interaction features (e.g., 'loan_amnt' / 'person_income') and apply log transforms to skewed numeric features identified in EDA.\n",
            "2. Advanced Imbalance Handling: Compare the current `class_weight` method against a pipeline using SMOTE, especially for the Neural Network (MLP) which performed poorly without it.\n",
            "3. Deeper Tuning: Use RandomizedSearchCV on a wider grid for the individual components of the Voting Ensemble.\n",
            "4. Model Interpretability: Use SHAP values on the final Voting Ensemble (or its best component, XGBoost) to explain key drivers, moving beyond the simple coefficients of the baseline [cite: 343-385].\n",
            "5. Final Submission: Train the best, fully-tuned pipeline (the Voting Ensemble) on the *entire* training dataset ('train.csv') and generate predictions for the 'test.csv' file.\n",
            "\n",
            "=== Checkpoint 2 Script Complete ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ======================================= \n",
        "# Loan Approval Prediction – Pitch Assets\n",
        "# EDA, preprocessing, and baseline models\n",
        "# =======================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# File paths\n",
        "TRAIN_PATH = \"/content/drive/My Drive/Data Corsair/playground_series_s4e10/train.csv\"\n",
        "TEST_PATH  = \"/content/drive/My Drive/Data Corsair/playground_series_s4e10/test.csv\"\n",
        "\n",
        "# Directory for plots\n",
        "FIG_DIR = \"/content/drive/My Drive/Data Corsair/loan_pitch_figures\"\n",
        "os.makedirs(FIG_DIR, exist_ok=True)\n",
        "\n",
        "def save_fig(path):\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(TRAIN_PATH)\n",
        "test  = pd.read_csv(TEST_PATH)\n",
        "\n",
        "# Class balance\n",
        "plt.figure(figsize=(6,4))\n",
        "train[\"loan_status\"].value_counts().sort_index().plot(kind=\"bar\")\n",
        "plt.title(\"Class Balance: loan_status\")\n",
        "plt.xlabel(\"loan_status\")\n",
        "plt.ylabel(\"Count\")\n",
        "save_fig(os.path.join(FIG_DIR, \"class_balance_loan_status.png\"))\n",
        "\n",
        "# Missing values per column\n",
        "plt.figure(figsize=(8,4))\n",
        "train.isna().sum().sort_values(ascending=False).plot(kind=\"bar\")\n",
        "plt.title(\"Missing Values per Column\")\n",
        "plt.ylabel(\"Count\")\n",
        "save_fig(os.path.join(FIG_DIR, \"missingness_per_column.png\"))\n",
        "\n",
        "# Numeric feature distributions (clipped to 1–99 percentiles to reduce extreme outliers)\n",
        "numeric_cols = [c for c in train.select_dtypes(include=np.number).columns if c not in [\"id\",\"loan_status\"]]\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    vals = train[col].dropna()\n",
        "    low, high = np.percentile(vals,[1,99])\n",
        "    vals = vals.clip(low,high)\n",
        "    plt.hist(vals, bins=30)\n",
        "    plt.title(f\"Distribution: {col}\")\n",
        "    save_fig(os.path.join(FIG_DIR, f\"hist_{col}.png\"))\n",
        "\n",
        "# Categorical feature distributions\n",
        "categorical_cols = [\"person_home_ownership\",\"loan_intent\",\"loan_grade\",\"cb_person_default_on_file\"]\n",
        "for col in categorical_cols:\n",
        "    if col in train.columns:\n",
        "        plt.figure(figsize=(7,4))\n",
        "        train[col].value_counts().plot(kind=\"bar\")\n",
        "        plt.title(f\"Distribution: {col}\")\n",
        "        save_fig(os.path.join(FIG_DIR, f\"bar_{col}.png\"))\n",
        "\n",
        "# Correlation heatmap for numeric features\n",
        "corr = train[numeric_cols+[\"loan_status\"]].corr()\n",
        "plt.figure(figsize=(7,6))\n",
        "plt.imshow(corr, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
        "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
        "plt.yticks(range(len(corr.index)), corr.index)\n",
        "plt.colorbar()\n",
        "plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "save_fig(os.path.join(FIG_DIR, \"correlation_heatmap_numeric.png\"))\n",
        "\n",
        "# Simple target–feature relationships\n",
        "if \"loan_percent_income\" in train.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    train.boxplot(column=\"loan_percent_income\", by=\"loan_status\")\n",
        "    plt.suptitle(\"\")\n",
        "    plt.title(\"loan_percent_income by loan_status\")\n",
        "    save_fig(os.path.join(FIG_DIR,\"box_loan_percent_income_by_status.png\"))\n",
        "\n",
        "if \"loan_grade\" in train.columns:\n",
        "    train.groupby(\"loan_grade\")[\"loan_status\"].mean().plot(kind=\"bar\")\n",
        "    plt.title(\"Approval Rate by Loan Grade\")\n",
        "    save_fig(os.path.join(FIG_DIR,\"approval_rate_by_loan_grade.png\"))\n",
        "\n",
        "if \"loan_intent\" in train.columns:\n",
        "    train.groupby(\"loan_intent\")[\"loan_status\"].mean().plot(kind=\"bar\")\n",
        "    plt.title(\"Approval Rate by Loan Intent\")\n",
        "    save_fig(os.path.join(FIG_DIR,\"approval_rate_by_loan_intent.png\"))\n",
        "\n",
        "# Baseline preprocessing and logistic regression check\n",
        "X = train.drop(columns=[\"loan_status\"])\n",
        "y = train[\"loan_status\"]\n",
        "\n",
        "num_features = [c for c in numeric_cols if c in X.columns]\n",
        "cat_features = [c for c in categorical_cols if c in X.columns]\n",
        "\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", StandardScaler(), num_features),\n",
        "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_features)\n",
        "])\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"preprocess\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=500))\n",
        "])\n",
        "\n",
        "X_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)\n",
        "pipe.fit(X_train,y_train)\n",
        "probs = pipe.predict_proba(X_valid)[:,1]\n",
        "auc = roc_auc_score(y_valid, probs)\n",
        "\n",
        "summary = {\n",
        "    \"train_shape\": train.shape,\n",
        "    \"test_shape\": test.shape,\n",
        "    \"numeric_features\": num_features,\n",
        "    \"categorical_features\": cat_features,\n",
        "    \"baseline_auc\": auc\n",
        "}\n",
        "with open(os.path.join(FIG_DIR,\"readiness_summary.json\"),\"w\") as f:\n",
        "    json.dump(summary,f,indent=2)\n",
        "\n",
        "print(\"Baseline AUC (LogReg):\", auc)\n",
        "print(\"Figures and summaries saved in\", FIG_DIR)\n",
        "\n",
        "# =======================================\n",
        "# PROJECT CHECKPOINT 2 EXTENSION (V3)\n",
        "# (Adds neural net, SVM, tuning, and ensembles)\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n=== Running Checkpoint 2 (v3) ===\")\n",
        "\n",
        "# Extra imports for Checkpoint 2\n",
        "import time\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "# Additional models\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except ImportError:\n",
        "    print(\"XGBoost not installed. Please run: pip install xgboost\")\n",
        "    XGBClassifier = None\n",
        "\n",
        "# Directory for Checkpoint 2 plots and tables\n",
        "FIG_DIR_CP2 = \"/content/drive/My Drive/Data Corsair/loan_checkpoint2_figures\"\n",
        "os.makedirs(FIG_DIR_CP2, exist_ok=True)\n",
        "print(f\"Checkpoint 2 artifacts will be saved in: {FIG_DIR_CP2}\")\n",
        "\n",
        "# =======================================\n",
        "# 1. Enhanced preprocessing with imputation\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n1. Building Enhanced Preprocessing Pipeline...\")\n",
        "\n",
        "# Numeric: impute median, then scale\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Categorical: impute mode, then one-hot encode\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "\n",
        "# Combined preprocessor used in all later models\n",
        "preprocess_cp2 = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, num_features),\n",
        "        ('cat', categorical_transformer, cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# =======================================\n",
        "# 2. Define the set of models to compare\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n2. Defining 9 models for comparison...\")\n",
        "\n",
        "# Class-weight scaling for XGBoost (based on CP1 y_train)\n",
        "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        max_iter=1000, class_weight='balanced', random_state=42\n",
        "    ),\n",
        "    'k-Nearest Neighbors (k-NN)': KNeighborsClassifier(),\n",
        "    'Support Vector Machine (SVM)': SVC(\n",
        "        probability=True, class_weight='balanced', random_state=42\n",
        "    ),\n",
        "    'Gaussian Naive Bayes': GaussianNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier(\n",
        "        class_weight='balanced', random_state=42\n",
        "    ),\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        class_weight='balanced', random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
        "    'Neural Network (MLP)': MLPClassifier(\n",
        "        random_state=42,\n",
        "        max_iter=500,\n",
        "        early_stopping=True,\n",
        "        hidden_layer_sizes=(100, 50)\n",
        "    )\n",
        "}\n",
        "\n",
        "if XGBClassifier:\n",
        "    models['XGBoost'] = XGBClassifier(\n",
        "        scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1, eval_metric='logloss'\n",
        "    )\n",
        "else:\n",
        "    print(\"Skipping XGBoost model as it is not installed.\")\n",
        "\n",
        "print(f\"Defined {len(models)} models.\")\n",
        "\n",
        "# =======================================\n",
        "# 3. Baseline comparison for all models\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n3. Running baseline model comparison (this may take a minute)...\")\n",
        "\n",
        "baseline_results = []\n",
        "roc_curves = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Full pipeline: shared preprocessing + model\n",
        "    pipe = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', model)\n",
        "    ])\n",
        "\n",
        "    # Fit\n",
        "    pipe.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions and probabilities\n",
        "    preds = pipe.predict(X_valid)\n",
        "    probs = pipe.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "    # Metrics\n",
        "    auc = roc_auc_score(y_valid, probs)\n",
        "    f1 = f1_score(y_valid, preds)\n",
        "    precision = precision_score(y_valid, preds)\n",
        "    recall = recall_score(y_valid, preds)\n",
        "    accuracy = accuracy_score(y_valid, preds)\n",
        "\n",
        "    # ROC curve data\n",
        "    fpr, tpr, _ = roc_curve(y_valid, probs)\n",
        "    roc_curves[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc}\n",
        "\n",
        "    # Store metrics for table\n",
        "    baseline_results.append({\n",
        "        \"Model\": name,\n",
        "        \"AUC\": auc,\n",
        "        \"F1\": f1,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"Accuracy\": accuracy\n",
        "    })\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"  ... {name} complete. AUC: {auc:.4f} (Time: {end_time - start_time:.2f}s)\")\n",
        "\n",
        "# Table of baseline results\n",
        "baseline_results_df = pd.DataFrame(baseline_results).sort_values(\n",
        "    by=\"AUC\", ascending=False\n",
        ")\n",
        "\n",
        "# Save baseline results\n",
        "baseline_results_df.to_csv(\n",
        "    os.path.join(FIG_DIR_CP2, \"baseline_model_comparison.csv\"), index=False\n",
        ")\n",
        "\n",
        "print(\"\\n--- Baseline Model Comparison (Sorted by AUC) ---\")\n",
        "print(baseline_results_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "print(\"\\nANALYSIS: XGBoost and RF are top performers.\")\n",
        "print(\"The default MLP is strong but likely needs SMOTE and tuning.\")\n",
        "\n",
        "# =======================================\n",
        "# 4. Hyperparameter tuning and soft-voting ensemble\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n4. Proposing improvements: Hyperparameter tuning for top 2 models...\")\n",
        "\n",
        "# Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# We tune RF and XGB and then build an ensemble using them\n",
        "tuned_estimators_for_ensemble = {}   # Estimator definitions for the ensemble\n",
        "tuned_pipelines_for_eval = []        # Fitted pipelines for evaluation\n",
        "\n",
        "# 4a. Random Forest tuning\n",
        "if 'Random Forest' in models:\n",
        "    print(\"\\n--- Tuning Random Forest (this may take a few minutes) ---\")\n",
        "    pipe_rf = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "\n",
        "    param_grid_rf = {\n",
        "        'clf__n_estimators': [100, 200],\n",
        "        'clf__max_depth': [10, 20],\n",
        "        'clf__min_samples_leaf': [2, 4]\n",
        "    }\n",
        "\n",
        "    grid_rf = GridSearchCV(\n",
        "        pipe_rf, param_grid_rf, cv=cv, scoring='roc_auc', verbose=1, n_jobs=-1\n",
        "    )\n",
        "    grid_rf.fit(X_train, y_train)\n",
        "    print(f\"Best RF Params: {grid_rf.best_params_}\")\n",
        "\n",
        "    # Store fitted pipeline for evaluation\n",
        "    tuned_pipelines_for_eval.append({\n",
        "        \"Model\": \"Tuned Random Forest\",\n",
        "        \"estimator\": grid_rf.best_estimator_\n",
        "    })\n",
        "    # Store classifier-only config for the ensemble\n",
        "    rf_best_params_clean = {k.replace('clf__', ''): v for k, v in grid_rf.best_params_.items()}\n",
        "    tuned_estimators_for_ensemble['rf'] = RandomForestClassifier(\n",
        "        **rf_best_params_clean, class_weight='balanced', random_state=42, n_jobs=-1\n",
        "    )\n",
        "\n",
        "# 4b. XGBoost tuning\n",
        "if 'XGBoost' in models:\n",
        "    print(\"\\n--- Tuning XGBoost (this may take a few minutes) ---\")\n",
        "    pipe_xgb = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', XGBClassifier(\n",
        "            scale_pos_weight=scale_pos_weight, random_state=42, n_jobs=-1, eval_metric='logloss'\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "    param_grid_xgb = {\n",
        "        'clf__n_estimators': [100, 200],\n",
        "        'clf__learning_rate': [0.05, 0.1],\n",
        "        'clf__max_depth': [3, 5]\n",
        "    }\n",
        "\n",
        "    grid_xgb = GridSearchCV(\n",
        "        pipe_xgb, param_grid_xgb, cv=cv, scoring='roc_auc', verbose=1, n_jobs=-1\n",
        "    )\n",
        "    grid_xgb.fit(X_train, y_train)\n",
        "    print(f\"Best XGB Params: {grid_xgb.best_params_}\")\n",
        "\n",
        "    # Store fitted pipeline for evaluation\n",
        "    tuned_pipelines_for_eval.append({\n",
        "        \"Model\": \"Tuned XGBoost\",\n",
        "        \"estimator\": grid_xgb.best_estimator_\n",
        "    })\n",
        "    # Store classifier-only config for the ensemble\n",
        "    xgb_best_params_clean = {k.replace('clf__', ''): v for k, v in grid_xgb.best_params_.items()}\n",
        "    tuned_estimators_for_ensemble['xgb'] = XGBClassifier(\n",
        "        **xgb_best_params_clean, scale_pos_weight=scale_pos_weight,\n",
        "        random_state=42, n_jobs=-1, eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "# 4c. Soft voting ensemble using tuned models + logistic regression\n",
        "print(\"\\n4c. Proposing improvement: Soft Voting Ensemble...\")\n",
        "\n",
        "# Add baseline logistic regression to the ensemble members\n",
        "tuned_estimators_for_ensemble['lr'] = LogisticRegression(\n",
        "    max_iter=1000, class_weight='balanced', random_state=42\n",
        ")\n",
        "\n",
        "# Build ensemble only if tuned RF and XGB are available\n",
        "if 'rf' in tuned_estimators_for_ensemble and 'xgb' in tuned_estimators_for_ensemble:\n",
        "    voting_classifier = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('lr', tuned_estimators_for_ensemble['lr']),\n",
        "            ('rf', tuned_estimators_for_ensemble['rf']),\n",
        "            ('xgb', tuned_estimators_for_ensemble['xgb'])\n",
        "        ],\n",
        "        voting='soft',\n",
        "        weights=[1, 2, 2]\n",
        "    )\n",
        "\n",
        "    pipe_voting = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', voting_classifier)\n",
        "    ])\n",
        "\n",
        "    print(\"Fitting Voting Classifier (this may take a minute)...\")\n",
        "    pipe_voting.fit(X_train, y_train)\n",
        "\n",
        "    tuned_pipelines_for_eval.append({\n",
        "        \"Model\": \"Voting Ensemble\",\n",
        "        \"estimator\": pipe_voting\n",
        "    })\n",
        "else:\n",
        "    print(\"Skipping Voting Ensemble: RF or XGB models were not tuned.\")\n",
        "\n",
        "# =======================================\n",
        "# 5. Evaluation of tuned models and ensemble\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n5. Evaluating tuned models and ensemble on validation set...\")\n",
        "\n",
        "final_metrics = []\n",
        "\n",
        "# Evaluate tuned models and ensemble\n",
        "for item in tuned_pipelines_for_eval:\n",
        "    name = item['Model']\n",
        "    model = item['estimator']\n",
        "\n",
        "    preds = model.predict(X_valid)\n",
        "    probs = model.predict_proba(X_valid)[:, 1]\n",
        "\n",
        "    auc = roc_auc_score(y_valid, probs)\n",
        "    f1 = f1_score(y_valid, preds)\n",
        "    precision = precision_score(y_valid, preds)\n",
        "    recall = recall_score(y_valid, preds)\n",
        "    accuracy = accuracy_score(y_valid, preds)\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_valid, probs)\n",
        "    roc_curves[name] = {'fpr': fpr, 'tpr': tpr, 'auc': auc}\n",
        "\n",
        "    final_metrics.append({\n",
        "        \"Model\": name,\n",
        "        \"AUC\": auc,\n",
        "        \"F1\": f1,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"Accuracy\": accuracy\n",
        "    })\n",
        "\n",
        "# Add selected baseline models to the same table for comparison\n",
        "for model_name in ['Logistic Regression', 'Random Forest', 'XGBoost', 'Neural Network (MLP)', 'Support Vector Machine (SVM)']:\n",
        "    if model_name in baseline_results_df['Model'].values:\n",
        "        final_metrics.append(\n",
        "            baseline_results_df[baseline_results_df['Model'] == model_name].to_dict('records')[0]\n",
        "        )\n",
        "\n",
        "# Final comparison table\n",
        "final_comparison_df = pd.DataFrame(final_metrics).sort_values(by=\"AUC\", ascending=False)\n",
        "\n",
        "# Save final comparison\n",
        "final_comparison_df.to_csv(\n",
        "    os.path.join(FIG_DIR_CP2, \"tuned_model_comparison.csv\"), index=False\n",
        ")\n",
        "\n",
        "print(\"\\n--- Final Comparison: Tuned, Ensemble, and Baseline ---\")\n",
        "print(final_comparison_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
        "print(\"\\nDELIVERABLE: Enhanced results show the Voting Ensemble provides\")\n",
        "print(\"the best and most robust performance.\")\n",
        "\n",
        "# Final ROC curve figure\n",
        "print(\"\\nGenerating final ROC curve plot...\")\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "models_to_plot = [\n",
        "    'Logistic Regression',\n",
        "    'Random Forest',\n",
        "    'Tuned Random Forest',\n",
        "    'XGBoost',\n",
        "    'Tuned XGBoost',\n",
        "    'Neural Network (MLP)',\n",
        "    'Support Vector Machine (SVM)',\n",
        "    'Voting Ensemble'\n",
        "]\n",
        "\n",
        "for name in models_to_plot:\n",
        "    if name in roc_curves:\n",
        "        data = roc_curves[name]\n",
        "        plt.plot(\n",
        "            data['fpr'],\n",
        "            data['tpr'],\n",
        "            label=f\"{name} (AUC = {data['auc']:.4f})\",\n",
        "            linewidth=2\n",
        "        )\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve Comparison: Tuned vs. Baseline Models', fontsize=16)\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True)\n",
        "\n",
        "# Save ROC plot to Checkpoint 2 directory\n",
        "save_fig(os.path.join(FIG_DIR_CP2, \"final_roc_curve_comparison.png\"))\n",
        "print(f\"Final ROC plot saved to {FIG_DIR_CP2}\")\n",
        "\n",
        "# =======================================\n",
        "# 6. Future work plan (for the project report)\n",
        "# =======================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"6. Future Plans for Final Project\")\n",
        "print(\"=\"*50)\n",
        "print(\"1. Advanced Feature Engineering: Create interaction features (e.g., 'loan_amnt' / 'person_income') and apply log transforms to skewed numeric features identified in EDA.\")\n",
        "print(\"2. Advanced Imbalance Handling: Compare the current `class_weight` method against a pipeline using SMOTE, especially for the Neural Network (MLP) which performed poorly without it.\")\n",
        "print(\"3. Deeper Tuning: Use RandomizedSearchCV on a wider grid for the individual components of the Voting Ensemble.\")\n",
        "print(\"4. Model Interpretability: Use SHAP values on the final Voting Ensemble (or its best component, XGBoost) to explain key drivers, moving beyond the simple coefficients of the baseline [cite: 343-385].\")\n",
        "print(\"5. Final Submission: Train the best, fully-tuned pipeline (the Voting Ensemble) on the *entire* training dataset ('train.csv') and generate predictions for the 'test.csv' file.\")\n",
        "print(\"\\n=== Checkpoint 2 Script Complete ===\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o9exHLbcZ_wI",
        "outputId": "d818c97b-3a34-47c6-c354-c0f81a37fd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ADDITIONAL EVALUATION: Extra Metrics for Main Models\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Logistic Regression (baseline-style) ===\n",
            "ROC-AUC:            0.9062\n",
            "Accuracy:           0.8453\n",
            "Balanced Accuracy:  0.8409\n",
            "Precision:          0.4753\n",
            "Recall:             0.8347\n",
            "F1:                 0.6057\n",
            "PR-AUC:             0.6994\n",
            "Confusion matrix [tn, fp, fn, tp]: 8520, 1539, 276, 1394\n",
            "\n",
            "=== Random Forest (tuned) ===\n",
            "ROC-AUC:            0.9350\n",
            "Accuracy:           0.9460\n",
            "Balanced Accuracy:  0.8602\n",
            "Precision:          0.8613\n",
            "Recall:             0.7401\n",
            "F1:                 0.7961\n",
            "PR-AUC:             0.8458\n",
            "Confusion matrix [tn, fp, fn, tp]: 9860, 199, 434, 1236\n",
            "\n",
            "=== XGBoost (tuned) ===\n",
            "ROC-AUC:            0.9545\n",
            "Accuracy:           0.9268\n",
            "Balanced Accuracy:  0.8879\n",
            "Precision:          0.7055\n",
            "Recall:             0.8335\n",
            "F1:                 0.7642\n",
            "PR-AUC:             0.8761\n",
            "Confusion matrix [tn, fp, fn, tp]: 9478, 581, 278, 1392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Voting Ensemble ===\n",
            "ROC-AUC:            0.9479\n",
            "Accuracy:           0.9357\n",
            "Balanced Accuracy:  0.8761\n",
            "Precision:          0.7644\n",
            "Recall:             0.7928\n",
            "F1:                 0.7784\n",
            "PR-AUC:             0.8673\n",
            "Confusion matrix [tn, fp, fn, tp]: 9651, 408, 346, 1324\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ADDITIONAL EVALUATION: Extra Metrics for Main Models\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "def evaluate_model(name, model, X_train, X_val, y_train, y_val, threshold=0.5):\n",
        "    # Fit model (safe to re-fit; uses same train split)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_prob = model.predict_proba(X_val)[:, 1]\n",
        "    y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "    roc_auc = roc_auc_score(y_val, y_prob)\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    bal_acc = balanced_accuracy_score(y_val, y_pred)\n",
        "    prec = precision_score(y_val, y_pred)\n",
        "    rec = recall_score(y_val, y_pred)\n",
        "    f1 = f1_score(y_val, y_pred)\n",
        "    prec_curve, rec_curve, _ = precision_recall_curve(y_val, y_prob)\n",
        "    pr_auc = auc(rec_curve, prec_curve)\n",
        "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
        "\n",
        "    print(f\"\\n=== {name} ===\")\n",
        "    print(f\"ROC-AUC:            {roc_auc:.4f}\")\n",
        "    print(f\"Accuracy:           {acc:.4f}\")\n",
        "    print(f\"Balanced Accuracy:  {bal_acc:.4f}\")\n",
        "    print(f\"Precision:          {prec:.4f}\")\n",
        "    print(f\"Recall:             {rec:.4f}\")\n",
        "    print(f\"F1:                 {f1:.4f}\")\n",
        "    print(f\"PR-AUC:             {pr_auc:.4f}\")\n",
        "    print(f\"Confusion matrix [tn, fp, fn, tp]: {tn}, {fp}, {fn}, {tp}\")\n",
        "\n",
        "    return {\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"accuracy\": acc,\n",
        "        \"balanced_accuracy\": bal_acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"tn\": tn, \"fp\": fp, \"fn\": fn, \"tp\": tp\n",
        "    }\n",
        "\n",
        "# Pipelines for main models using the enhanced preprocessing\n",
        "lr_pipe = Pipeline(steps=[\n",
        "    ('preprocess', preprocess_cp2),\n",
        "    ('clf', LogisticRegression(\n",
        "        max_iter=1000, class_weight='balanced', random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "\n",
        "rf_pipe = None\n",
        "xgb_pipe = None\n",
        "\n",
        "if 'rf' in tuned_estimators_for_ensemble:\n",
        "    rf_pipe = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', tuned_estimators_for_ensemble['rf'])\n",
        "    ])\n",
        "\n",
        "if 'xgb' in tuned_estimators_for_ensemble:\n",
        "    xgb_pipe = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', tuned_estimators_for_ensemble['xgb'])\n",
        "    ])\n",
        "\n",
        "# Voting ensemble pipeline exists as `pipe_voting`\n",
        "voting_pipe = pipe_voting\n",
        "\n",
        "# Run evaluations\n",
        "results_lr = evaluate_model(\"Logistic Regression (baseline-style)\", lr_pipe,\n",
        "                            X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "if rf_pipe is not None:\n",
        "    results_rf = evaluate_model(\"Random Forest (tuned)\", rf_pipe,\n",
        "                                X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "if xgb_pipe is not None:\n",
        "    results_xgb = evaluate_model(\"XGBoost (tuned)\", xgb_pipe,\n",
        "                                 X_train, X_valid, y_train, y_valid)\n",
        "\n",
        "results_ens = evaluate_model(\"Voting Ensemble\", voting_pipe,\n",
        "                             X_train, X_valid, y_train, y_valid)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4weUjqWNk0gX",
        "outputId": "f7a72765-9ffa-4a36-8bc7-01b8f63ebf42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "ADDITIONAL EVALUATION: 5-fold Cross-Validated ROC-AUC\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression 5-fold ROC-AUC: 0.9034 ± 0.0049\n",
            "XGBoost 5-fold ROC-AUC: 0.9530 ± 0.0040\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ADDITIONAL EVALUATION: 5-fold Cross-Validated ROC-AUC\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Logistic Regression CV\n",
        "lr_cv = Pipeline(steps=[\n",
        "    ('preprocess', preprocess_cp2),\n",
        "    ('clf', LogisticRegression(\n",
        "        max_iter=1000, class_weight='balanced', random_state=42\n",
        "    ))\n",
        "])\n",
        "lr_scores = cross_val_score(lr_cv, X, y, cv=cv, scoring='roc_auc')\n",
        "print(f\"Logistic Regression 5-fold ROC-AUC: {lr_scores.mean():.4f} ± {lr_scores.std():.4f}\")\n",
        "\n",
        "# XGBoost CV \n",
        "if XGBClassifier is not None:\n",
        "    xgb_cv = Pipeline(steps=[\n",
        "        ('preprocess', preprocess_cp2),\n",
        "        ('clf', XGBClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=4,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            objective=\"binary:logistic\",\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            eval_metric=\"logloss\",\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ])\n",
        "    xgb_scores = cross_val_score(xgb_cv, X, y, cv=cv, scoring='roc_auc')\n",
        "    print(f\"XGBoost 5-fold ROC-AUC: {xgb_scores.mean():.4f} ± {xgb_scores.std():.4f}\")\n",
        "else:\n",
        "    print(\"XGBoost not installed, skipping 5-fold ROC-AUC for XGBoost.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEiI9Kgos-Ct",
        "outputId": "0fab1bfa-8563-461d-81f4-9e8d98442e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "7. Novel Framework: Stacked Cost-Sensitive Loan Approval Model\n",
            "==================================================\n",
            "Fitting stacked model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best cost-sensitive threshold: 0.970\n",
            "Max expected profit (relative units): 584.00\n",
            "\n",
            "=== Stacked Cost-Sensitive Model (Validation) ===\n",
            "threshold           : 0.9700\n",
            "tp                  : 1136\n",
            "fp                  : 57\n",
            "fn                  : 534\n",
            "tn                  : 10002\n",
            "roc_auc             : 0.9524\n",
            "accuracy            : 0.9496\n",
            "balanced_accuracy   : 0.8373\n",
            "precision           : 0.9522\n",
            "recall              : 0.6802\n",
            "f1                  : 0.7936\n",
            "Cost-sensitive profit curve saved to /content/drive/My Drive/Data Corsair/loan_checkpoint2_figures\n"
          ]
        }
      ],
      "source": [
        "# =======================================\n",
        "# 7. Stacked Cost-Sensitive Loan Approval Model\n",
        "# =======================================\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"7. Novel Framework: Stacked Cost-Sensitive Loan Approval Model\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7.1 Stacking meta-learner (model-side novelty)\n",
        "# ---------------------------------------------------------\n",
        "# Base estimators: tuned RF/XGB when available, plus LR and MLP\n",
        "base_estimators = []\n",
        "\n",
        "# Base Logistic Regression\n",
        "base_estimators.append(\n",
        "    ('lr_base', LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ))\n",
        ")\n",
        "\n",
        "# Base Random Forest (tuned from earlier GridSearch)\n",
        "if 'rf' in tuned_estimators_for_ensemble:\n",
        "    base_estimators.append(('rf_base', tuned_estimators_for_ensemble['rf']))\n",
        "\n",
        "# Base XGBoost (tuned) if available\n",
        "if XGBClassifier is not None and 'xgb' in tuned_estimators_for_ensemble:\n",
        "    base_estimators.append(('xgb_base', tuned_estimators_for_ensemble['xgb']))\n",
        "\n",
        "# Base MLP\n",
        "mlp_base = MLPClassifier(\n",
        "    random_state=42,\n",
        "    max_iter=500,\n",
        "    early_stopping=True,\n",
        "    hidden_layer_sizes=(100, 50)\n",
        ")\n",
        "base_estimators.append(('mlp_base', mlp_base))\n",
        "\n",
        "# Meta-learner: Logistic Regression on stacked probabilities + passthrough features\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=base_estimators,\n",
        "    final_estimator=LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    ),\n",
        "    stack_method='predict_proba',\n",
        "    passthrough=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stack_pipe = Pipeline(steps=[\n",
        "    ('preprocess', preprocess_cp2),\n",
        "    ('clf', stack_clf)\n",
        "])\n",
        "\n",
        "print(\"Fitting stacked model...\")\n",
        "stack_pipe.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7.2 Cost-sensitive decision rule (decision-side)\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Assumptions:\n",
        "#   y = 1 -> \"good\" (approve)\n",
        "#   y = 0 -> \"bad\"  (reject)\n",
        "# Simple profit model (arbitrary units):\n",
        "#   TP: +1.0   (approve good)\n",
        "#   FP: -5.0   (approve bad)\n",
        "#   FN: -0.5   (reject good)\n",
        "#   TN:  0.0   (reject bad)\n",
        "def cost_sensitive_sweep(y_true, y_prob,\n",
        "                         gain_tp=1.0,\n",
        "                         loss_fp=-5.0,\n",
        "                         loss_fn=-0.5,\n",
        "                         gain_tn=0.0,\n",
        "                         n_thresholds=99):\n",
        "    thresholds = np.linspace(0.01, 0.99, n_thresholds)\n",
        "    rows = []\n",
        "    for thr in thresholds:\n",
        "        y_pred = (y_prob >= thr).astype(int)\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "        profit = tp * gain_tp + fp * loss_fp + fn * loss_fn + tn * gain_tn\n",
        "        rows.append({\n",
        "            \"threshold\": thr,\n",
        "            \"tp\": tp,\n",
        "            \"fp\": fp,\n",
        "            \"fn\": fn,\n",
        "            \"tn\": tn,\n",
        "            \"profit\": profit\n",
        "        })\n",
        "    df = pd.DataFrame(rows)\n",
        "    best = df.loc[df['profit'].idxmax()].copy()\n",
        "    return df, best\n",
        "\n",
        "# Validation probabilities from stacked model\n",
        "stack_val_prob = stack_pipe.predict_proba(X_valid)[:, 1]\n",
        "cs_df, best_cs = cost_sensitive_sweep(y_valid, stack_val_prob)\n",
        "\n",
        "best_thr = float(best_cs[\"threshold\"])\n",
        "print(f\"\\nBest cost-sensitive threshold: {best_thr:.3f}\")\n",
        "print(f\"Max expected profit (relative units): {best_cs['profit']:.2f}\")\n",
        "\n",
        "# Evaluate stacked model at the selected threshold\n",
        "stack_cs_pred = (stack_val_prob >= best_thr).astype(int)\n",
        "tn, fp, fn, tp = confusion_matrix(y_valid, stack_cs_pred).ravel()\n",
        "\n",
        "stack_cs_metrics = {\n",
        "    \"threshold\": best_thr,\n",
        "    \"tp\": int(tp),\n",
        "    \"fp\": int(fp),\n",
        "    \"fn\": int(fn),\n",
        "    \"tn\": int(tn),\n",
        "    \"roc_auc\": roc_auc_score(y_valid, stack_val_prob),\n",
        "    \"accuracy\": accuracy_score(y_valid, stack_cs_pred),\n",
        "    \"balanced_accuracy\": balanced_accuracy_score(y_valid, stack_cs_pred),\n",
        "    \"precision\": precision_score(y_valid, stack_cs_pred),\n",
        "    \"recall\": recall_score(y_valid, stack_cs_pred),\n",
        "    \"f1\": f1_score(y_valid, stack_cs_pred)\n",
        "}\n",
        "\n",
        "print(\"\\n=== Stacked Cost-Sensitive Model (Validation) ===\")\n",
        "for k, v in stack_cs_metrics.items():\n",
        "    if isinstance(v, float):\n",
        "        print(f\"{k:20s}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{k:20s}: {v}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 7.3 Threshold–profit curve for stacked model\n",
        "# ---------------------------------------------------------\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(cs_df[\"threshold\"], cs_df[\"profit\"])\n",
        "plt.xlabel(\"Decision Threshold\")\n",
        "plt.ylabel(\"Expected Profit (relative units)\")\n",
        "plt.title(\"Stacked Model: Cost-Sensitive Threshold Sweep\")\n",
        "plt.grid(True)\n",
        "save_fig(os.path.join(FIG_DIR_CP2, \"stacked_cost_sensitive_profit_curve.png\"))\n",
        "print(f\"Cost-sensitive profit curve saved to {FIG_DIR_CP2}\")\n",
        "\n",
        "# Save metrics for later use in the report\n",
        "with open(os.path.join(FIG_DIR_CP2, \"stacked_cost_sensitive_metrics.json\"), \"w\") as f:\n",
        "    json.dump(stack_cs_metrics, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYjc07x9vTsh",
        "outputId": "38c7917c-8473-4f53-eb98-d2e9bdd17b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "8. Novel Framework: Segment-Specific Expert Ensemble\n",
            "==================================================\n",
            "Segment split on loan_percent_income median: 0.1400\n",
            "Train segment sizes -> low_ratio: 24343, high_ratio: 22573\n",
            "Valid segment sizes -> low_ratio: 5951, high_ratio: 5778\n",
            "\n",
            "Fitting low-risk expert on low_ratio segment...\n",
            "Fitting high-risk expert on high_ratio segment...\n",
            "\n",
            "Best segment-specific thresholds:\n",
            "  low_ratio segment threshold  : 0.700\n",
            "  high_ratio segment threshold : 0.900\n",
            "  Max expected profit (relative units): 553.00\n",
            "\n",
            "=== Segment-Specific Expert Ensemble (Validation) ===\n",
            "thr_low               : 0.7000\n",
            "thr_high              : 0.9000\n",
            "tp                    : 1112\n",
            "fp                    : 56\n",
            "fn                    : 558\n",
            "tn                    : 10003\n",
            "roc_auc               : 0.9354\n",
            "accuracy              : 0.9477\n",
            "balanced_accuracy     : 0.8302\n",
            "precision             : 0.9521\n",
            "recall                : 0.6659\n",
            "f1                    : 0.7837\n",
            "Segmented expert profit heatmap saved to /content/drive/My Drive/Data Corsair/loan_checkpoint2_figures\n"
          ]
        }
      ],
      "source": [
        "# =======================================\n",
        "# 8. Segment-specific expert ensemble\n",
        "# =======================================\n",
        "from sklearn.base import clone\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"8. Novel Framework: Segment-Specific Expert Ensemble\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8.1 Defining segmentation rule on raw feature space\n",
        "# ---------------------------------------------------------\n",
        "# We split applicants by loan_percent_income into two segments:\n",
        "#   - low_ratio  : loan_percent_income <= median\n",
        "#   - high_ratio : loan_percent_income > median\n",
        "if \"loan_percent_income\" not in X_train.columns:\n",
        "    raise ValueError(\n",
        "        \"loan_percent_income not found in X_train; \"\n",
        "        \"cannot build segment-specific expert ensemble.\"\n",
        "    )\n",
        "\n",
        "income_ratio_median = X_train[\"loan_percent_income\"].median()\n",
        "print(f\"Segment split on loan_percent_income median: {income_ratio_median:.4f}\")\n",
        "\n",
        "train_low_mask = X_train[\"loan_percent_income\"] <= income_ratio_median\n",
        "train_high_mask = ~train_low_mask\n",
        "\n",
        "val_low_mask = X_valid[\"loan_percent_income\"] <= income_ratio_median\n",
        "val_high_mask = ~val_low_mask\n",
        "\n",
        "print(f\"Train segment sizes -> low_ratio: {train_low_mask.sum()}, \"\n",
        "      f\"high_ratio: {train_high_mask.sum()}\")\n",
        "print(f\"Valid segment sizes -> low_ratio: {val_low_mask.sum()}, \"\n",
        "      f\"high_ratio: {val_high_mask.sum()}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8.2 Segment-specific expert models\n",
        "# ---------------------------------------------------------\n",
        "\n",
        "# Low-ratio expert: tuned RF if available, otherwise a reasonable RF\n",
        "if 'rf' in tuned_estimators_for_ensemble:\n",
        "    clf_low = clone(tuned_estimators_for_ensemble['rf'])\n",
        "else:\n",
        "    clf_low = RandomForestClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=20,\n",
        "        min_samples_leaf=2,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "# High-ratio expert: tuned XGB if available, else a strong XGB, else logistic regression\n",
        "if XGBClassifier is not None and 'xgb' in tuned_estimators_for_ensemble:\n",
        "    clf_high = clone(tuned_estimators_for_ensemble['xgb'])\n",
        "elif XGBClassifier is not None:\n",
        "    clf_high = XGBClassifier(\n",
        "        n_estimators=200,\n",
        "        max_depth=4,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        objective=\"binary:logistic\",\n",
        "        scale_pos_weight=scale_pos_weight,\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "else:\n",
        "    clf_high = LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "low_expert = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_cp2),\n",
        "    (\"clf\", clf_low)\n",
        "])\n",
        "\n",
        "high_expert = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocess_cp2),\n",
        "    (\"clf\", clf_high)\n",
        "])\n",
        "\n",
        "print(\"\\nFitting low-risk expert on low_ratio segment...\")\n",
        "low_expert.fit(X_train[train_low_mask], y_train[train_low_mask])\n",
        "\n",
        "print(\"Fitting high-risk expert on high_ratio segment...\")\n",
        "high_expert.fit(X_train[train_high_mask], y_train[train_high_mask])\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8.3 Helper to get segment-wise probabilities\n",
        "# ---------------------------------------------------------\n",
        "def segmented_proba(X_df):\n",
        "    \"\"\"\n",
        "    Route each sample to its segment-specific expert and\n",
        "    return a single probability vector plus the segment masks.\n",
        "    \"\"\"\n",
        "    proba = np.zeros(len(X_df), dtype=float)\n",
        "    low_mask = X_df[\"loan_percent_income\"] <= income_ratio_median\n",
        "    high_mask = ~low_mask\n",
        "\n",
        "    if low_mask.any():\n",
        "        proba[low_mask] = low_expert.predict_proba(X_df[low_mask])[:, 1]\n",
        "    if high_mask.any():\n",
        "        proba[high_mask] = high_expert.predict_proba(X_df[high_mask])[:, 1]\n",
        "\n",
        "    return proba, low_mask, high_mask\n",
        "\n",
        "val_proba_seg, val_low_mask, val_high_mask = segmented_proba(X_valid)\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8.4 Segment-specific cost-sensitive threshold search\n",
        "# ---------------------------------------------------------\n",
        "# Same business cost model as before:\n",
        "#   TP: +1.0   (approve good)\n",
        "#   FP: -5.0   (approve bad)\n",
        "#   FN: -0.5   (reject good)\n",
        "#   TN:  0.0   (reject bad)\n",
        "def segmented_cost_sensitive_sweep(\n",
        "    y_true,\n",
        "    proba,\n",
        "    low_mask,\n",
        "    high_mask,\n",
        "    gain_tp=1.0,\n",
        "    loss_fp=-5.0,\n",
        "    loss_fn=-0.5,\n",
        "    gain_tn=0.0,\n",
        "    thresholds=np.linspace(0.05, 0.95, 19)\n",
        "):\n",
        "    rows = []\n",
        "    for thr_low in thresholds:\n",
        "        for thr_high in thresholds:\n",
        "            # Segment-specific thresholds:\n",
        "            # low_ratio  -> thr_low\n",
        "            # high_ratio -> thr_high\n",
        "            thr_vec = np.where(low_mask, thr_low, thr_high)\n",
        "            y_pred = (proba >= thr_vec).astype(int)\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "            profit = tp * gain_tp + fp * loss_fp + fn * loss_fn + tn * gain_tn\n",
        "            rows.append({\n",
        "                \"thr_low\": thr_low,\n",
        "                \"thr_high\": thr_high,\n",
        "                \"tp\": tp,\n",
        "                \"fp\": fp,\n",
        "                \"fn\": fn,\n",
        "                \"tn\": tn,\n",
        "                \"profit\": profit\n",
        "            })\n",
        "    df = pd.DataFrame(rows)\n",
        "    best = df.loc[df[\"profit\"].idxmax()].copy()\n",
        "    return df, best\n",
        "\n",
        "seg_cs_df, seg_best = segmented_cost_sensitive_sweep(\n",
        "    y_valid.values,\n",
        "    val_proba_seg,\n",
        "    val_low_mask.values,\n",
        "    val_high_mask.values\n",
        ")\n",
        "\n",
        "print(\"\\nBest segment-specific thresholds:\")\n",
        "print(f\"  low_ratio segment threshold  : {seg_best['thr_low']:.3f}\")\n",
        "print(f\"  high_ratio segment threshold : {seg_best['thr_high']:.3f}\")\n",
        "print(f\"  Max expected profit (relative units): {seg_best['profit']:.2f}\")\n",
        "\n",
        "# Evaluate with the best thresholds\n",
        "thr_low_best = float(seg_best[\"thr_low\"])\n",
        "thr_high_best = float(seg_best[\"thr_high\"])\n",
        "\n",
        "thr_vec_val = np.where(val_low_mask.values, thr_low_best, thr_high_best)\n",
        "seg_pred_val = (val_proba_seg >= thr_vec_val).astype(int)\n",
        "\n",
        "tn, fp, fn, tp = confusion_matrix(y_valid, seg_pred_val).ravel()\n",
        "segmented_metrics = {\n",
        "    \"thr_low\": thr_low_best,\n",
        "    \"thr_high\": thr_high_best,\n",
        "    \"tp\": int(tp),\n",
        "    \"fp\": int(fp),\n",
        "    \"fn\": int(fn),\n",
        "    \"tn\": int(tn),\n",
        "    \"roc_auc\": roc_auc_score(y_valid, val_proba_seg),\n",
        "    \"accuracy\": accuracy_score(y_valid, seg_pred_val),\n",
        "    \"balanced_accuracy\": balanced_accuracy_score(y_valid, seg_pred_val),\n",
        "    \"precision\": precision_score(y_valid, seg_pred_val),\n",
        "    \"recall\": recall_score(y_valid, seg_pred_val),\n",
        "    \"f1\": f1_score(y_valid, seg_pred_val)\n",
        "}\n",
        "\n",
        "print(\"\\n=== Segment-Specific Expert Ensemble (Validation) ===\")\n",
        "for k, v in segmented_metrics.items():\n",
        "    if isinstance(v, float):\n",
        "        print(f\"{k:22s}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{k:22s}: {v}\")\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 8.5 Save metrics and profit surface\n",
        "# ---------------------------------------------------------\n",
        "with open(os.path.join(FIG_DIR_CP2, \"segmented_expert_metrics.json\"), \"w\") as f:\n",
        "    json.dump(segmented_metrics, f, indent=2)\n",
        "\n",
        "seg_cs_df.to_csv(\n",
        "    os.path.join(FIG_DIR_CP2, \"segmented_threshold_profit_grid.csv\"),\n",
        "    index=False\n",
        ")\n",
        "\n",
        "# Heatmap of profit over (thr_low, thr_high)\n",
        "pivot = seg_cs_df.pivot(index=\"thr_low\", columns=\"thr_high\", values=\"profit\")\n",
        "plt.figure(figsize=(7, 6))\n",
        "im = plt.imshow(pivot.values, origin=\"lower\", aspect=\"auto\")\n",
        "plt.xticks(\n",
        "    range(len(pivot.columns)),\n",
        "    [f\"{c:.2f}\" for c in pivot.columns],\n",
        "    rotation=90\n",
        ")\n",
        "plt.yticks(\n",
        "    range(len(pivot.index)),\n",
        "    [f\"{r:.2f}\" for r in pivot.index]\n",
        ")\n",
        "plt.xlabel(\"High-Risk Threshold (thr_high)\")\n",
        "plt.ylabel(\"Low-Risk Threshold (thr_low)\")\n",
        "plt.title(\"Segmented Expert Ensemble: Profit Surface\")\n",
        "plt.colorbar(im, label=\"Expected Profit\")\n",
        "save_fig(os.path.join(FIG_DIR_CP2, \"segmented_expert_profit_heatmap.png\"))\n",
        "print(f\"Segmented expert profit heatmap saved to {FIG_DIR_CP2}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
